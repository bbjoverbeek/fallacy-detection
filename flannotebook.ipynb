{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-28T14:23:46.865166Z",
     "start_time": "2024-05-28T14:23:45.158295Z"
    }
   },
   "source": [
    "import transformers\n",
    "from transformers import pipeline\n",
    "import torch"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T14:23:49.696317Z",
     "start_time": "2024-05-28T14:23:49.607175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if CUDA is available and set the device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "e473fcc957690086",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T14:23:51.297996Z",
     "start_time": "2024-05-28T14:23:51.292954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to load the model and tokenizer\n",
    "def load_model(model_name=\"google/flan-t5-xl\"):\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "    model = transformers.T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "    model.to(device)\n",
    "    return model, tokenizer\n",
    "\n",
    "# Function to set up the pipeline\n",
    "def setup_pipeline(model, tokenizer):\n",
    "    pipe = pipeline('text2text-generation', model=model, tokenizer=tokenizer, device=0)\n",
    "    return pipe\n",
    "\n",
    "# Function to generate text based on prompts\n",
    "def generate_text(prompt, pipe, max_tokens=100):\n",
    "    response = pipe(\n",
    "        prompt, \n",
    "        max_length=max_tokens, \n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,  # Adjust temperature as needed\n",
    "    )\n",
    "    return response[0]['generated_text']"
   ],
   "id": "115271647fad3681",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-05-28T14:23:53.031884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"google/flan-t5-xl\"\n",
    "model, tokenizer = load_model(model_name)\n",
    "pipe = setup_pipeline(model, tokenizer)"
   ],
   "id": "eceb9a7ec28dd2e8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0531bb0c06e94664b923e31552a3cab6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6543e3d3f6ef4912a921c290984d8fd4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e4628ca04a664fb9bc7c9aabe8501a03"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2ab0e6ade8374581a926a3d52fffe1e3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "469f49f8717045cca728e440b793fb96"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/53.0k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "976b557148894167820d93bbfd05c9ec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2dd73fc9f061455f8c155c2b62af9511"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.45G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f3c68c8204ad4bed9bcdea79a240fcbe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "INSTRUCTION_KEY = \"### Instruction:\"\n",
    "RESPONSE_KEY = \"### Response:\"\n",
    "INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "PROMPT_FOR_GENERATION_FORMAT = \"\"\"{intro}\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "{response_key}\n",
    "\"\"\".format(\n",
    "    intro=INTRO_BLURB,\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    ")\n",
    "\n",
    "# Function to read the prompt from a file\n",
    "def read_prompt_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        prompt = file.read().strip()\n",
    "    return prompt\n",
    "\n",
    "# Define the path to the prompt file\n",
    "prompt_file_path = 'prompt.txt'\n",
    "\n",
    "# Read the prompt from the file\n",
    "instruction = read_prompt_from_file(prompt_file_path)\n",
    "\n",
    "# Format the prompt according to the specified format\n",
    "formatted_prompt = PROMPT_FOR_GENERATION_FORMAT.format(instruction=instruction)\n",
    "\n",
    "# Print the formatted prompt to verify\n",
    "print(\"Formatted Prompt:\\n\", formatted_prompt)\n",
    "\n",
    "# Generate the text\n",
    "output = generate_text(formatted_prompt, pipe, max_tokens=300)\n",
    "\n",
    "# Print the generated text\n",
    "print(\"Generated Text:\\n\", output)"
   ],
   "id": "cddb0802ba08a08d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
