{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-28T15:10:13.049569Z",
     "start_time": "2024-05-28T15:10:11.399038Z"
    }
   },
   "source": [
    "import transformers\n",
    "from transformers import pipeline\n",
    "import torch"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T15:10:13.110494Z",
     "start_time": "2024-05-28T15:10:13.051155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if CUDA is available and set the device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "e473fcc957690086",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T15:10:13.115980Z",
     "start_time": "2024-05-28T15:10:13.111797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to load the model and tokenizer\n",
    "def load_model(model_name=\"google/flan-t5-xl\"):\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "    model = transformers.T5ForConditionalGeneration.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "    model.to(device)\n",
    "    return model, tokenizer\n",
    "\n",
    "# Function to set up the pipeline\n",
    "def setup_pipeline(model, tokenizer):\n",
    "    pipe = pipeline('text2text-generation', model=model, tokenizer=tokenizer, device=0)\n",
    "    return pipe\n",
    "\n",
    "# Function to generate text based on prompts\n",
    "def generate_text(prompt, pipe, max_tokens=100):\n",
    "    response = pipe(\n",
    "        prompt, \n",
    "        max_length=max_tokens, \n",
    "        num_return_sequences=1,\n",
    "    )\n",
    "    return response[0]['generated_text']"
   ],
   "id": "115271647fad3681",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T15:10:23.076705Z",
     "start_time": "2024-05-28T15:10:13.118101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"google/flan-t5-xl\"\n",
    "model, tokenizer = load_model(model_name)\n",
    "pipe = setup_pipeline(model, tokenizer)"
   ],
   "id": "eceb9a7ec28dd2e8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c68cfdd54d794e5fbfeef955f850c0e2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T15:11:38.314367Z",
     "start_time": "2024-05-28T15:11:38.172790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "INSTRUCTION_KEY = \"### Instruction:\"\n",
    "RESPONSE_KEY = \"### Response:\"\n",
    "\n",
    "INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "PROMPT_FOR_GENERATION_FORMAT = \"\"\"{intro}\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "{response_key}\n",
    "\"\"\".format(\n",
    "    intro=INTRO_BLURB,\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    ")\n",
    "\n",
    "# Function to read the prompt from a file\n",
    "def read_prompt_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        prompt = file.read().strip()\n",
    "    return prompt\n",
    "\n",
    "# Define the path to the prompt file\n",
    "prompt_file_path = 'prompt.txt'\n",
    "\n",
    "# Read the prompt from the file\n",
    "instruction = read_prompt_from_file(prompt_file_path)\n",
    "\n",
    "# Format the prompt according to the specified format\n",
    "formatted_prompt = PROMPT_FOR_GENERATION_FORMAT.format(instruction=instruction)\n",
    "\n",
    "# Print the formatted prompt to verify\n",
    "print(\"Formatted Prompt:\\n\", formatted_prompt)\n",
    "\n",
    "# Generate the text\n",
    "output = generate_text(formatted_prompt, pipe, max_tokens=3000)\n",
    "\n",
    "# Print the generated text\n",
    "print(\"Generated Text:\\n\", output)"
   ],
   "id": "cddb0802ba08a08d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted Prompt:\n",
      " Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "The potential fallacious argument types are:\n",
      "AA slippery slope\n",
      "BB ad hominem\n",
      "CC appeal to (false) authority\n",
      "DD X appeal to majority\n",
      "EE No fallacy\n",
      "\n",
      "Which one of these 5 fallacious argument types does the following text contain?\n",
      "\"There are two distinguished authors on arms control in this country -- there are many others, but two that I want to cite tonight. One is Strobe Talbott in his classic book, \"Deadly Gambits.'' The other is John Neuhaus, who's one of the most distinguished arms control specialists in our country. Both said that this administration turned down the \"walk in the woods'' agreement first, and that would have been a perfect agreement from the standpoint of the United States in Europe and our security.\"\n",
      "\n",
      "Please choose an answer form AA,BB,CC,DD or EE.\n",
      "Before identifying the fallacy, explain your reasoning thoroughly. Your explanation should clarify why the specific fallacy applies to the given statement. This step is crucial!\n",
      "If you do not explain your reasoning, you will not receive credit for this question.\n",
      "### Response:\n",
      "\n",
      "Generated Text:\n",
      " CC\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
