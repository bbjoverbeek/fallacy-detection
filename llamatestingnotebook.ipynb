{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-28T10:50:45.493778Z",
     "start_time": "2024-05-28T10:50:45.485857Z"
    }
   },
   "source": [
    "# Import libraries\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from huggingface_hub import login"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T10:50:46.845265Z",
     "start_time": "2024-05-28T10:50:46.708313Z"
    }
   },
   "cell_type": "code",
   "source": "login(token=\"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\")",
   "id": "c44507932dd34ce4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T10:50:52.190554Z",
     "start_time": "2024-05-28T10:50:52.184854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Check if CUDA is available and set the device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ],
   "id": "8339a59701774358",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T10:50:55.317572Z",
     "start_time": "2024-05-28T10:50:55.308738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to load the model and tokenizer\n",
    "def load_model(model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\"):\n",
    "    config = transformers.AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(model_name, config=config, torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "    model.to(device)\n",
    "    return model, tokenizer\n",
    "\n",
    "# Function to set up the pipeline\n",
    "def setup_pipeline(model, tokenizer):\n",
    "    pipe = pipeline('text-generation', model=model, tokenizer=tokenizer, device=0)\n",
    "    return pipe\n",
    "\n",
    "# Function to generate text based on prompts\n",
    "def generate_text(prompt, pipe, max_tokens=100):\n",
    "    response = pipe(\n",
    "        prompt, \n",
    "        max_length=max_tokens, \n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    return response[0]['generated_text']"
   ],
   "id": "c9b72611751a370d",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T10:52:35.503621Z",
     "start_time": "2024-05-28T10:50:56.590741Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "model, tokenizer = load_model(model_name)\n",
    "pipe = setup_pipeline(model, tokenizer)"
   ],
   "id": "e445713a07471cc5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eef9da1012424135b911b59f51315a5f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "77d0b44beb3f4e458ca17e9b030fa522"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c4ce3c206be34a8590c0f1a0b36a6895"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f2c1b9942dd84b569fa03513468491f8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1d3bcbe65c8445f08528645ad5c9915d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "267a371158f249f39267c7718e487a66"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8161082179834fa4ae5ac709ed93bed2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "39d70bdb74624b4aa4bdc4ab263d9fe9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ec917ff0430641b4bcce5f0fb1ae726b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2de88aaeb67e44e790f0400f253da4d6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c7f9bb74447144039e46b004148610c1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "43541415ceef43a2baffc6fe551ffd40"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-28T11:02:19.757698Z",
     "start_time": "2024-05-28T11:02:11.849259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define constants\n",
    "INSTRUCTION_KEY = \"### Instruction:\"\n",
    "RESPONSE_KEY = \"### Response:\"\n",
    "INTRO_BLURB = \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "PROMPT_FOR_GENERATION_FORMAT = \"\"\"{intro}\n",
    "{instruction_key}\n",
    "{instruction}\n",
    "{response_key}\n",
    "\"\"\".format(\n",
    "    intro=INTRO_BLURB,\n",
    "    instruction_key=INSTRUCTION_KEY,\n",
    "    instruction=\"{instruction}\",\n",
    "    response_key=RESPONSE_KEY,\n",
    ")\n",
    "\n",
    "# Function to read the prompt from a file\n",
    "def read_prompt_from_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        prompt = file.read().strip()\n",
    "    return prompt\n",
    "\n",
    "# Define the path to the prompt file\n",
    "prompt_file_path = 'prompt.txt'\n",
    "\n",
    "# Read the prompt from the file\n",
    "instruction = read_prompt_from_file(prompt_file_path)\n",
    "\n",
    "# Format the prompt according to the specified format\n",
    "formatted_prompt = PROMPT_FOR_GENERATION_FORMAT.format(instruction=instruction)\n",
    "\n",
    "# Print the formatted prompt to verify\n",
    "# print(\"Formatted Prompt:\\n\", formatted_prompt)\n",
    "\n",
    "# Generate the text\n",
    "output = generate_text(formatted_prompt, pipe, max_tokens=500)\n",
    "\n",
    "# Print the generated text\n",
    "print(\"Generated Text:\\n\", output)"
   ],
   "id": "3c956f63df2d105e",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      " Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "### Instruction:\n",
      "The potential fallacious argument types are:\n",
      "- slippery slope\n",
      "- ad hominem\n",
      "- appeal to (false) authority\n",
      "- X appeal to majority\n",
      "- No fallacy\n",
      "\n",
      "Which one of these 5 fallacious argument types does the following text contain?\n",
      "\"We heard Senator Kerry say the other night that there ought to be some kind of global test before U.S. troops are deployed preemptively to protect the United States. That's part of a track record that goes back to the 1970s when he ran for Congress the first time and said troops should not be deployed without U.N. approval. Then, in the mid-'80s, he ran on the basis of cutting most of our major defense programs. In 1991, he voted against Desert Storm.  It's a consistent pattern over time of always being on the wrong side of defense issues.\"\n",
      "\n",
      "Before identifying the fallacy, explain your reasoning thoroughly. Your explanation should clarify why the specific fallacy applies to the given statement. This step is crucial!\n",
      "### Response:\n",
      "The text contains an ad hominem fallacy. The author of the text is attacking Senator Kerry's character by highlighting his past statements and actions, rather than addressing the merits of his argument about the global test. The author is attempting to discredit Kerry by implying that his views on defense issues are consistently wrong, rather than engaging with the specific idea of a global test.\n",
      "\n",
      "My reasoning is as follows: The text does not engage with Kerry's argument about the global test, nor does it provide any evidence or counterarguments to refute his idea. Instead, the author is focusing on Kerry's past statements and actions, trying to portray him as someone who has always been wrong on defense issues. This is an ad hominem attack, as it targets Kerry's character rather than his argument. The author is not addressing the substance of Kerry's argument, but rather trying to undermine his credibility by highlighting his past mistakes. This type of fallacious reasoning is an ad hominem attack, as it distracts from the issue at hand and focuses on personal attacks rather than engaging with the argument itself. Therefore, the text contains an ad hominem fallacy.\n"
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
